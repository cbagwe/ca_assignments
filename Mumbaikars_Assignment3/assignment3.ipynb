{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1495f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import statements\n",
    "import json\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48d3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training dataset \n",
    "with open(\"train-data-prepared.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "# get training dataset \n",
    "with open(\"val-data-prepared.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "# get testing dataset \n",
    "with open(\"val-data-prepared.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca5dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spacy object\n",
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "#create Stemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ae5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_ids', 'train_posts', 'train_label', 'val_ids', 'val_posts', 'val_label', 'test_ids', 'test_posts', 'test_label'])\n"
     ]
    }
   ],
   "source": [
    "entire_dataset = {\n",
    "    'train_ids': [thread[\"id\"] for thread in train_data],\n",
    "    'train_posts': [thread[\"preceding_posts\"] for thread in train_data],\n",
    "    'train_label': [thread[\"label\"] for thread in train_data],\n",
    "    \n",
    "    'val_ids': [thread[\"id\"] for thread in val_data],\n",
    "    'val_posts': [thread[\"preceding_posts\"] for thread in val_data],\n",
    "    'val_label': [thread[\"label\"] for thread in val_data],\n",
    "    \n",
    "    'test_ids': [thread[\"id\"] for thread in test_data],\n",
    "    'test_posts': [thread[\"preceding_posts\"] for thread in test_data],\n",
    "    'test_label': [thread[\"label\"] for thread in test_data],\n",
    "}\n",
    "\n",
    "print(entire_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f8a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for thread in val_data:\n",
    "#    authors = []\n",
    "#    for comment in thread[\"preceding_posts\"]:\n",
    "#        if comment[\"author_name\"] not in authors:\n",
    "#            authors.append(comment[\"author_name\"])\n",
    "#    \n",
    "#    if len(authors) >= 2:\n",
    "#        print(thread[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85d4fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ass', 'idiot', 'fuck', 'shit', 'racist']\n",
      "[' ', 'right', 'women', 'non', 'sexual', 'creatures', 'use', 'prostitutes', 'think', 'vastly', 'overestimate', 'number', 'women', 'pay', 'sex']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation, space, urls from text\n",
    "def clean_text(text):\n",
    "    parsed_text = nlp_english(text)\n",
    "    clean_text = []\n",
    "    for token in parsed_text:\n",
    "        stop_flag = (token.is_punct or token.is_space or  \n",
    "                 token.like_url or token.is_stop)\n",
    "        if (not stop_flag):\n",
    "            clean_text.append(re.sub('[^A-Za-z0-9]+', ' ',token.text.lower()))\n",
    "            \n",
    "    return clean_text\n",
    "\n",
    "def stem_text(text):\n",
    "    return [stemmer.stem(word) for word in clean_text(text)]\n",
    "\n",
    "print(stem_text(\"ass idiot fuck shit racist \"))\n",
    "print(clean_text(\"> a) right, because women are non-sexual creatures who would never use prostitutes themselves\\n\\ni think you vastly overestimate the number of women that pay for sex...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baff7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if the OP has addressed the other person in some way\n",
    "def count_insults(text):\n",
    "    insult_words = [\"ass\", \"idiot\", \"fuck\", \"shit\", \"racist\"]\n",
    "    counter = 0\n",
    "    for word in text:\n",
    "        if word in insult_words:\n",
    "            counter = counter + 1\n",
    "            \n",
    "    return counter\n",
    "        \n",
    "#print(check_insults(clean_text(\"> a) right, because women are non-sexual creatures who would never use prostitutes themselves\\n\\ni think you vastly overestimate the number of women that pay for sex...\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8ed80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should return one feature vector for one string\n",
    "# features -> Author's turn, length of argument, insults, sentiment\n",
    "def gather_data(thread):\n",
    "    returnObj = {}\n",
    "    for i in range(len(thread[\"preceding_posts\"])):\n",
    "        comment_data = {}\n",
    "        comment = thread[\"preceding_posts\"][i]\n",
    "        # clean text\n",
    "        comment_data[\"text\"] = stem_text(comment[\"body\"])\n",
    "        # author's identity: \n",
    "        # Assuming 3 dialogues, the authors will have indices 0,1,2\n",
    "        # 0,2 would be the OP\n",
    "        # 1 would be the reply\n",
    "        #comment_data[\"author_turn_vec\"] = [i % 2]\n",
    "        # Length just in case of Godwin's Law\n",
    "        comment_data[\"char_length_vec\"] = [len(\"\".join(clean_text(comment[\"body\"])))]\n",
    "        # check for some common insults\n",
    "        comment_data[\"insults_vec\"] = [count_insults(comment_data[\"text\"])]\n",
    "        # get sentiment\n",
    "        sentiment = TextBlob(' '.join(comment_data[\"text\"])).sentiment\n",
    "        comment_data[\"sentiment\"] =  [sentiment.polarity, sentiment.subjectivity]\n",
    "        feature_vec = comment_data[\"char_length_vec\"] + comment_data[\"insults_vec\"] + comment_data[\"sentiment\"]\n",
    "        \n",
    "        returnObj[\" \".join(comment_data[\"text\"])] = feature_vec\n",
    "        \n",
    "    return returnObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7204dd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'  right women non sexual creatur use prostitut think vastli overestim number women pay sex': [83, 0, 0.39285714285714285, 0.6845238095238095], 'illeg realiti vs  propos realiti op thread comment qwert': [55, 0, 0.0, 0.0], 'live nation complet legal pay sex surround nation legal guess woman pay sex notic illeg stop men go prostitut bunchanumb edit context world nation consider liber attitud sex': [171, 0, 0.1787878787878788, 0.3]}\n",
      "\n",
      "{'make prostitut legal make difficult polic found repeatedli countri world women brought countri question legal justif job legal found nevada germani australia': [155, 0, 0.02500000000000001, 0.39999999999999997], 'interest read sourc exemplifi refer cop question ask immigr statu warrant voluntari': [91, 0, 0.0, 0.0]}\n",
      "\n",
      "{'specif elect presid focu discuss unfortun find book read subject have move call earli decad earli year read american histori class colleg come today cite passag deal issu   justifi use elector colleg hamilton focus argument deal colleg oppos direct elect explain role gener populac elect presid hamilton argu sens peopl elect elector colleg process final lie elector hamilton note men capabl analyz qualiti adapt station act circumst favor deliber judici combin reason induc proper govern choic direct elect presid left select voter elector indirect elect justifi hamilton republ serv system allow certain type person elect presid prevent individu unfit varieti reason posit chief execut countri basic dumb dumb trust need men capabl analyz qualiti adapt station act circumst favor deliber judici combin reason induc proper govern choic read paper kinda funni relev trump elector colleg trust major lot safeguard hamilton envis eras technolog': [1023, 0, 0.2580059523809524, 0.5414880952380953], 'link wikipedia direct text origin claim back case actual like read text snippet actual federalist paper claim read litterslli read paper 0 assert back': [148, 0, 0.02, 0.12], 'wiki figur want synopsi right read comprehens bad written day think hamilton say   immedi elect men capabl analyz qualiti adapt station act circumst favor deliber judici combin reason induc proper govern choic small number person select fellow citizen gener mass like possess inform discern requisit complic investig think talk rural farmer   process elect afford moral certainti offic presid fall lot man emin degre endow requisit qualif talent low intrigu littl art popular suffic elev man honor singl state requir talent differ kind merit establish esteem confid union consider portion necessari success candid distinguish offic presid unit state basic say process weed idiot win popular contest mass state  presid forese cabl news social media unfortun honestli read think messag': [843, 1, 0.20383597883597881, 0.49689153439153444]}\n",
      "\n",
      "{'  real properti logic absolut necess regist logic environ properti ownership exist need know properti belong person need know build hous land actual belong expand properti 10 feet 10 feet properti belong neighbor allow report neighbor trespass need prove properti line need drawn line need regist govern enforc properti relat law reli   opinion heller wrongli decid wonder approach right protect 1st 4th 5th amend expans individualist approach singl 2nd restrict collect approach rememb grammat speak present participl militia restrict mean independ claus grammat right state pre exist uncondit school elector necessari secur free state right peopl read book shall infring equival sentenc grammar mean interpret mean read book act collect capac elector   area registr court seen wrong okay have regist print govern identifi   wit crime natur suspect reason wit confirm disconfirm perpetr relev evid hope crimin regist firearm rememb forc punish fail find firearm build evid search warrant search find firearm specif firearm attempt match hammer chainsaw deadli implement': [1110, 0, 0.0014030612244898069, 0.46926020408163266], '  environ properti ownership exist need know properti belong person need know build hous land actual belong expand properti 10 feet 10 feet properti belong neighbor allow report neighbor trespass need prove properti line need drawn line need regist govern enforc properti relat law reli actual lot societi central properti registr simpli individu document wit exampl person properti kept document exampl ancient jew similar system mediev area case logic necess system posit neg system case system posit outweigh neg paragraph actual examin major reason properti regist modern societi liabil happen properti want know respons second scale difficult scale system peopl person deed larg scale big citi like properti tax locat properti tax central record real estat necessari administ system fourth modern zone hard central note reason logic necess appar posit system case outweigh benefit privaci   wonder approach right protect 1st 4th 5th amend expans individualist approach singl 2nd restrict collect approach histori intent amend matter militia nation guard privat citizen   rememb grammat speak present participl militia restrict mean independ claus grammat right state pre exist uncondit   school elector necessari secur free state right peopl read book shall infring equival sentenc grammar mean interpret mean read book act collect capac elector problem grammar note actual peopl amend use phrase certainli collect connot essenti problem grammat liter 200 year pretti agre intend mean 40 50 year taken expans read 2nd amend serious help read dissent heller     wit crime natur suspect reason wit confirm disconfirm perpetr relev evid hope crimin regist firearm   rememb forc punish fail interest disagre event current preced appli peopl crimin record problem logic respons amend narrow abolish 2nd amend outright   find firearm build evid search warrant search find firearm specif firearm attempt match hammer chainsaw deadli implement firearm substanti deadli like crimin activ hammer chainsaw relev type evid get warrant live regist firearm relev type': [2158, 0, -0.02997039724980901, 0.3533963585434174]}\n",
      "\n",
      "{'defend multin greedi tri nestl idea human right water life knew perfectli babyformula result dead infant dead continu stop public outrag public outrag west': [156, 0, -0.019047619047619053, 0.26150793650793647], '  defend multin greedi sentenc sens go guess sure point tri water human right natur resourc': [98, 0, 0.2619047619047619, 0.5082010582010582], '  sentenc sens eli5 think multin suffer greed exempt   water human right natur resourc claim human money right water realis person need water surviv': [143, 0, 0.14285714285714285, 0.31785714285714284]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print features for some 10 tuples\n",
    "for thread in train_data[:5]:\n",
    "    print(gather_data(thread))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56d556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1936/1936 [08:39<00:00,  3.72it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [01:01<00:00,  4.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [01:01<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "entire_dataset[\"train_prep\"] = [gather_data(thread) for thread in tqdm(train_data)]\n",
    "\n",
    "entire_dataset[\"val_prep\"] = [gather_data(thread) for thread in tqdm(val_data)]\n",
    "\n",
    "entire_dataset[\"test_prep\"] = [gather_data(thread) for thread in tqdm(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1668cd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef concatAllStringForBoW(listOfDict):\\n    return_obj = []\\n    for d_dict in listOfDict:\\n        return_obj.append(\" \".join(list(d_dict.keys())))\\n        \\n    return return_obj      \\n\\nprint(len(concatAllStringForBoW(entire_dataset[\"train_prep\"])))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def concatAllStringForBoW(listOfDict):\n",
    "    return_obj = []\n",
    "    for d_dict in listOfDict:\n",
    "        return_obj.append(\" \".join(list(d_dict.keys())))\n",
    "        \n",
    "    return return_obj      \n",
    "\n",
    "print(len(concatAllStringForBoW(entire_dataset[\"train_prep\"])))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf200ace",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_bow_input = concatAllStringForBoW(entire_dataset[\"train_prep\"])\\nval_bow_input = concatAllStringForBoW(entire_dataset[\"val_prep\"])\\ntest_bow_input = concatAllStringForBoW(entire_dataset[\"test_prep\"])\\n\\nprint(len(train_bow_input))\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_bow_input = concatAllStringForBoW(entire_dataset[\"train_prep\"])\n",
    "val_bow_input = concatAllStringForBoW(entire_dataset[\"val_prep\"])\n",
    "test_bow_input = concatAllStringForBoW(entire_dataset[\"test_prep\"])\n",
    "\n",
    "print(len(train_bow_input))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312e5c31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvectorizer = CountVectorizer()\\n\\ntrain_bow = vectorizer.fit_transform(train_bow_input)\\nval_bow = vectorizer.transform(val_bow_input)\\ntest_bow = vectorizer.transform(test_bow_input)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "train_bow = vectorizer.fit_transform(train_bow_input)\n",
    "val_bow = vectorizer.transform(val_bow_input)\n",
    "test_bow = vectorizer.transform(test_bow_input)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa69d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_bow.toarray().tolist()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f4a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta_in_sentences(array, index, increasing_flag = False):\n",
    "    values = [x[index] for x in array]\n",
    "    if len(values) == 2:\n",
    "        ret_answer = values[0] < values[1]\n",
    "    else:\n",
    "        ret_answer1 = values[0] < values[1]\n",
    "        ret_answer2 = values[1] < values[2]\n",
    "        ret_answer = ret_answer1 & ret_answer2\n",
    "        \n",
    "    if increasing_flag:\n",
    "        return int(ret_answer)\n",
    "    else:\n",
    "        return int(not ret_answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d09b79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(get_delta_in_sentences([[1,2,3,5],[10,20,2,3],[7,22,1,4]],-1,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6736e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(ddict):\n",
    "    feature_vectors = list(ddict.values())\n",
    "    dot_product = dot(feature_vectors[0], feature_vectors[-1])\n",
    "    norms_product = (norm(feature_vectors[0])*norm(feature_vectors[-1]))\n",
    "    \n",
    "    if norms_product == 0:\n",
    "        cos_sim = 1\n",
    "    else:\n",
    "        cos_sim = dot_product/norms_product\n",
    "    \n",
    "    cos_diff = 1 - cos_sim\n",
    "    is_author_turn_next = 1 - (len(feature_vectors)%2)\n",
    "    count_dialogues = len(feature_vectors)\n",
    "    avg_length = np.average([x[1] for x in feature_vectors])\n",
    "    insults_count = np.sum([x[1] for x in feature_vectors])\n",
    "    increasing_insults = get_delta_in_sentences(feature_vectors, 1, True)\n",
    "    avg_polarity = np.average([x[-2] for x in feature_vectors])\n",
    "    stddev_polarity = np.std([x[-2] for x in feature_vectors])\n",
    "    avg_subjectivity = np.average([x[-1] for x in feature_vectors])\n",
    "    stddev_subjectivity = np.std([x[-1] for x in feature_vectors])\n",
    "    is_decreasing_polarity = get_delta_in_sentences(feature_vectors, -2)\n",
    "    is_increasing_polarity = get_delta_in_sentences(feature_vectors, -2, True)\n",
    "    is_decreasing_subjectivity = get_delta_in_sentences(feature_vectors, -1)\n",
    "    is_increasing_subjectivity = get_delta_in_sentences(feature_vectors, -1, True)\n",
    "    #print(ret_obj)\n",
    "    return [is_author_turn_next, count_dialogues, avg_length, \n",
    "        avg_polarity, stddev_polarity, avg_subjectivity, \n",
    "        stddev_subjectivity, is_decreasing_polarity, is_increasing_polarity, \n",
    "        is_decreasing_subjectivity, is_increasing_subjectivity, cos_diff, cos_sim]\n",
    "    \n",
    "#combine_vectors(entire_dataset['train_prep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e95aef7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1936/1936 [00:00<00:00, 2517.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 3785.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 5911.12it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = [combine_vectors(thread) for thread in tqdm(entire_dataset['train_prep'])]\n",
    "x_val = [combine_vectors(thread) for thread in tqdm(entire_dataset['val_prep'])]\n",
    "x_test = [combine_vectors(thread) for thread in tqdm(entire_dataset['test_prep'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d295078",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628975e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train2 = [x_train[i] + train_bow.toarray().tolist()[i] for i in tqdm(range(len(x_train)))]\n",
    "#x_val2 = [x_val[i] + val_bow.toarray().tolist()[i]  for i in tqdm(range(len(x_val)))]\n",
    "#x_test2 = [x_test[i] + test_bow.toarray().tolist()[i] for i in tqdm(range(len(x_test)))]\n",
    "\n",
    "#print(x_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aa02ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = entire_dataset[\"train_label\"]\n",
    "y_val = entire_dataset[\"val_label\"]\n",
    "y_test = entire_dataset[\"test_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852edfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "567e7924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "1936\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[1]))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68f2937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(verbose=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(verbose = True)\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be50f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf.predict(x_val)\n",
    "test_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eabd521d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for val data: 0.9922480620155039\n",
      "Accuracy for test data: 0.9922480620155039\n",
      "Precision val: 1.0\n",
      "Precision test: 1.0\n",
      "Recall val: 0.9844961240310077\n",
      "Recall test: 0.9844961240310077\n",
      "F1 score val: 0.9921875\n",
      "F1 score test: 0.9921875\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for val data:\",metrics.accuracy_score(y_val, val_pred))\n",
    "print(\"Accuracy for test data:\",metrics.accuracy_score(y_test, test_pred))\n",
    "\n",
    "print(\"Precision val:\",metrics.precision_score(y_val, val_pred))\n",
    "print(\"Precision test:\",metrics.precision_score(y_test, test_pred))\n",
    "\n",
    "print(\"Recall val:\",metrics.recall_score(y_val, val_pred))\n",
    "print(\"Recall test:\",metrics.recall_score(y_test, test_pred))\n",
    "\n",
    "print(\"F1 score val:\",metrics.f1_score(y_val, val_pred))\n",
    "print(\"F1 score test:\",metrics.f1_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1233c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random classification for baseline score\n",
    "\n",
    "#random_val = {t_id: random.randint(0,1) for t_id in val_ids}\n",
    "#random_test = {t_id: random.randint(0,1) for t_id in test_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c226fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"val-random-output.json\", \"w\") as f:\n",
    "#    json.dump(random_val, f)\n",
    "# get testing dataset \n",
    "#with open(\"test-random-output.json\", \"w\") as f:\n",
    "#    json.dump(random_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e3c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
