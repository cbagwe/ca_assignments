{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1495f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from better_profanity import profanity\n",
    "from nltk.stem import PorterStemmer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48d3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training dataset \n",
    "with open(\"train-data-prepared.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "# get training dataset \n",
    "with open(\"val-data-prepared.json\", \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "# get testing dataset \n",
    "with open(\"val-data-prepared.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca5dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spacy object\n",
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "#create Stemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ae5c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_ids', 'train_posts', 'train_label', 'val_ids', 'val_posts', 'val_label', 'test_ids', 'test_posts', 'test_label'])\n"
     ]
    }
   ],
   "source": [
    "entire_dataset = {\n",
    "    'train_ids': [thread[\"id\"] for thread in train_data],\n",
    "    'train_posts': [thread[\"preceding_posts\"] for thread in train_data],\n",
    "    'train_label': [thread[\"label\"] for thread in train_data],\n",
    "    \n",
    "    'val_ids': [thread[\"id\"] for thread in val_data],\n",
    "    'val_posts': [thread[\"preceding_posts\"] for thread in val_data],\n",
    "    'val_label': [thread[\"label\"] for thread in val_data],\n",
    "    \n",
    "    'test_ids': [thread[\"id\"] for thread in test_data],\n",
    "    'test_posts': [thread[\"preceding_posts\"] for thread in test_data],\n",
    "    'test_label': [thread[\"label\"] for thread in test_data],\n",
    "}\n",
    "\n",
    "print(entire_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f8a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for thread in val_data:\n",
    "#    authors = []\n",
    "#    for comment in thread[\"preceding_posts\"]:\n",
    "#        if comment[\"author_name\"] not in authors:\n",
    "#            authors.append(comment[\"author_name\"])\n",
    "#    \n",
    "#    if len(authors) >= 2:\n",
    "#        print(thread[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85d4fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation, space, urls from text\n",
    "def clean_text(text):\n",
    "    parsed_text = nlp_english(text)\n",
    "    clean_text = []\n",
    "    for token in parsed_text:\n",
    "        stop_flag = (token.is_punct or token.is_space or  \n",
    "                 token.like_url)\n",
    "        if (not stop_flag):\n",
    "            clean_text.append(re.sub('[^A-Za-z0-9]+', ' ',token.text.lower()))\n",
    "            \n",
    "    return clean_text\n",
    "\n",
    "def stem_text(text):\n",
    "    return [stemmer.stem(word) for word in clean_text(text)]\n",
    "\n",
    "#print(stem_text(\"ass idiot fuck shit racist \"))\n",
    "#print(stem_text(\"...because it's illegal in our reality, vs. the proposed reality that me, OP, and everyone else on this thread are commenting about.\\n\\nKeep up, qwert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baff7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if the OP has addressed the other person in some way\n",
    "def count_insults(text):\n",
    "    insult_words = [\"ass\", \"idiot\", \"fuck\", \"shit\"]\n",
    "    counter = 0\n",
    "    for word in text:\n",
    "        if word in insult_words:\n",
    "            counter = counter + 1\n",
    "            \n",
    "    return counter\n",
    "        \n",
    "#print(check_insults(clean_text(\"> a) right, because women are non-sexual creatures who would never use prostitutes themselves\\n\\ni think you vastly overestimate the number of women that pay for sex...\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8436f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def check_author_name(text, name):\n",
    "#    flag = 0\n",
    "#    for word in text:\n",
    "#        if name in word or word in name:\n",
    "#            flag = flag + 1\n",
    "#    return flag\n",
    "\n",
    "#print(check_author_name(['becaus', 'it', ' s', 'illeg', 'in', 'our', 'realiti', 'vs ', 'the', 'propos', 'realiti', 'that', 'me', 'op', 'and', 'everyon', 'els', 'on', 'thi', 'thread', 'are', 'comment', 'about', 'keep', 'up', 'qwert'],\"qwertx0815\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4dd1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_number(list_of_tuple, key):\n",
    "    counter = 0\n",
    "    for (word,tag) in list_of_tuple:\n",
    "        if key in tag:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "#B = stem_text(\"this is a disgusting attitude that glorifies those with power over those who have none. This is the heart of fascism\\n\\nOkay, so now we're getting into ad hominems. Got it. I find it interesting how I'm the one denouncing creeping authoritarianism and you're the one calling that fascist. I'm not convinced you know what fascism actually means.\\n\\n>You mean you won't get into it because then you would have to admit that you are defending the right of bigots to engage in prejudicial treatment of others, and that is morally indefensible.\\n\\nBigotry (noun): intolerance to those who hold different opinions from oneself.\\n\\n>I sincerely hope that you are made the victim of prejudice and bigotry, so that you can one day understand how disgusting and malignant your beliefs are.\\n\\nHmmm... Wishing ill will upon those who are different from you? Sounds a lot like how you described those bakers refusing to bake a cake for gay couples. Also seems to fit nicely into that definition of bigotry I listed above.\\n\\nBigotry comes in many forms. Thinking that anyone who isn't a neo-progressive liberal such as yourself is a bigot is one of those forms. I sincerely hope you take some time to reflect on how you view others with whom you disagree, because I can't see how anyone with so much animosity could possibly be happy.\\n\\nEnjoy your weekend! The weather's gorgeous here, I hope it's nice where you are too.\")\n",
    "\n",
    "#print(TextBlob(\" \".join(B)).tags)\n",
    "#print(get_number(TextBlob(\" \".join(B)).tags, 'NN'))\n",
    "#print(get_number(TextBlob(\" \".join(B)).tags, 'VBP'))\n",
    "#print(get_number(TextBlob(\" \".join(B)).tags, 'MD'))\n",
    "#print(get_number(TextBlob(\" \".join(B)).tags, 'PRP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8ed80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should return one feature vector for one string\n",
    "# features -> Author's turn, length of argument, insults, sentiment\n",
    "def gather_data(thread):\n",
    "    returnObj = {}\n",
    "    for i in range(len(thread[\"preceding_posts\"])):\n",
    "        comment_data = {}\n",
    "        comment = thread[\"preceding_posts\"][i]\n",
    "        # clean text\n",
    "        comment_data[\"text\"] = stem_text(comment[\"body\"])\n",
    "        # length just in case of Godwin's Law\n",
    "        comment_data[\"char_length_vec\"] = [len(\"\".join(comment_data[\"text\"]))]\n",
    "        # refer's to other user/author\n",
    "        #comment_data[\"refer_author\"] = [check_author_name(comment_data[\"text\"], thread[\"preceding_posts\"][1-i][\"author_name\"])]\n",
    "        # check for some common insults\n",
    "        comment_data[\"insults_vec\"] = [count_insults(comment_data[\"text\"])]\n",
    "        # calculate number of POS tags\n",
    "        sentence_tags = TextBlob(comment[\"body\"]).tags\n",
    "        comment_data[\"count_pos\"] = [get_number(sentence_tags, 'NN'), get_number(sentence_tags, 'VBP'), \n",
    "                                     get_number(sentence_tags, 'MD'), get_number(sentence_tags, 'PRP')]\n",
    "        # get sentiment\n",
    "        sentiment = TextBlob(' '.join(comment_data[\"text\"])).sentiment\n",
    "        comment_data[\"sentiment\"] =  [sentiment.polarity, sentiment.subjectivity]\n",
    "        feature_vec = comment_data[\"char_length_vec\"] + comment_data[\"insults_vec\"] + comment_data[\"sentiment\"] + comment_data[\"count_pos\"]\n",
    "        \n",
    "        returnObj[\" \".join(comment_data[\"text\"])] = feature_vec\n",
    "        \n",
    "    return returnObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7204dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print features for some 10 tuples\n",
    "#for thread in train_data[:5]:\n",
    "#    print(gather_data(thread))\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f56d556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1936/1936 [05:19<00:00,  6.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:38<00:00,  6.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 258/258 [00:37<00:00,  6.80it/s]\n"
     ]
    }
   ],
   "source": [
    "entire_dataset[\"train_prep\"] = [gather_data(thread) for thread in tqdm(train_data)]\n",
    "\n",
    "entire_dataset[\"val_prep\"] = [gather_data(thread) for thread in tqdm(val_data)]\n",
    "\n",
    "entire_dataset[\"test_prep\"] = [gather_data(thread) for thread in tqdm(test_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfa69d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train_bow.toarray().tolist()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a7d6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatAllStringForBoW(listOfDict):\n",
    "    return_obj = []\n",
    "    for d_dict in listOfDict:\n",
    "        return_obj.append(\" \".join(list(d_dict.keys())))\n",
    "        \n",
    "    return return_obj  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44b567b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_input = concatAllStringForBoW(entire_dataset[\"train_prep\"])\n",
    "val_bow_input = concatAllStringForBoW(entire_dataset[\"val_prep\"])\n",
    "test_bow_input = concatAllStringForBoW(entire_dataset[\"test_prep\"])\n",
    "\n",
    "#print(len(train_bow_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "524bea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_bow = vectorizer.fit_transform(train_bow_input).toarray().tolist()\n",
    "val_bow = vectorizer.transform(val_bow_input).toarray().tolist()\n",
    "test_bow = vectorizer.transform(test_bow_input).toarray().tolist()\n",
    "\n",
    "#print(len(train_bow[0])) #12565\n",
    "#print(len(train_bow)) #1936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f4a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_increased(array, index):\n",
    "    values = [x[index] for x in array]\n",
    "    ret_answer = values[0] < values[1]\n",
    "    return int(ret_answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6736e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(ddict):\n",
    "    feature_vectors = list(ddict.values())\n",
    "    \n",
    "    (insults_index, polar_index) = (1,2)\n",
    "    \n",
    "    dot_product = dot(feature_vectors[0], feature_vectors[-1])\n",
    "    norms_product = (norm(feature_vectors[0])*norm(feature_vectors[-1]))\n",
    "    if norms_product == 0:\n",
    "        cos_sim = 1\n",
    "    else:\n",
    "        cos_sim = dot_product/norms_product\n",
    "    \n",
    "    #cos_diff = 1 - cos_sim\n",
    "    avg_insults = np.average([x[insults_index] for x in feature_vectors])\n",
    "    is_increasing_insults = is_increased(feature_vectors, insults_index)\n",
    "    \n",
    "    avg_polarity = np.average([x[polar_index] for x in feature_vectors])\n",
    "    is_increasing_polarity = is_increased(feature_vectors, polar_index)\n",
    "    \n",
    "    return [avg_insults, is_increasing_insults, avg_polarity, is_increasing_polarity,  cos_sim]\n",
    "    \n",
    "#combine_vectors(entire_dataset['train_prep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e95aef7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1936/1936 [00:00<00:00, 9052.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 6448.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 10753.66it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = [combine_vectors(thread) for thread in tqdm(entire_dataset['train_prep'])]\n",
    "x_val = [combine_vectors(thread) for thread in tqdm(entire_dataset['val_prep'])]\n",
    "x_test = [combine_vectors(thread) for thread in tqdm(entire_dataset['test_prep'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f7a05e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0, 0.029999999999999992, 0, 0.9995706912786162]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_train[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f96bb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1936/1936 [00:00<00:00, 8161.23it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 6993.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 258/258 [00:00<00:00, 10746.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_train[i].extend(train_bow[i]) for i in tqdm(range(len(x_train))) ]\n",
    "[x_val[i].extend(val_bow[i]) for i in tqdm(range(len(x_val))) ]\n",
    "[x_test[i].extend(test_bow[i]) for i in tqdm(range(len(x_test))) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c598fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(x_train_temp[1])) 12570\n",
    "#print(len(x_train_temp)) 1936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aa02ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = entire_dataset[\"train_label\"]\n",
    "y_val = entire_dataset[\"val_label\"]\n",
    "y_test = entire_dataset[\"test_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f2937d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be50f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = clf.predict(x_val)\n",
    "test_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd521d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy for val data:\",metrics.accuracy_score(y_val, val_pred))\n",
    "print(\"Accuracy for test data:\",metrics.accuracy_score(y_test, test_pred))\n",
    "\n",
    "print(\"Precision val:\",metrics.precision_score(y_val, val_pred))\n",
    "print(\"Precision test:\",metrics.precision_score(y_test, test_pred))\n",
    "\n",
    "print(\"Recall val:\",metrics.recall_score(y_val, val_pred))\n",
    "print(\"Recall test:\",metrics.recall_score(y_test, test_pred))\n",
    "\n",
    "print(\"F1 score val:\",metrics.f1_score(y_val, val_pred))\n",
    "print(\"F1 score test:\",metrics.f1_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random classification for baseline score\n",
    "\n",
    "#random_val = {t_id: random.randint(0,1) for t_id in val_ids}\n",
    "#random_test = {t_id: random.randint(0,1) for t_id in test_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c226fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"val-random-output.json\", \"w\") as f:\n",
    "#    json.dump(random_val, f)\n",
    "# get testing dataset \n",
    "#with open(\"test-random-output.json\", \"w\") as f:\n",
    "#    json.dump(random_test, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
